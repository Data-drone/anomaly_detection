{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into ‘/usr/local/spark-3.0.1-bin-hadoop3.2/R/lib’\n",
      "(as ‘lib’ is unspecified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "install.packages(\"sparklyr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(sparklyr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc <- spark_connect(master = \"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Parquet Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "'data/s3_parquet_logs.c000'"
      ],
      "text/latex": [
       "'data/s3\\_parquet\\_logs.c000'"
      ],
      "text/markdown": [
       "'data/s3_parquet_logs.c000'"
      ],
      "text/plain": [
       "[1] \"data/s3_parquet_logs.c000\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system(\"ls data/*\", intern=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$master\n",
       "[1] \"local[16]\"\n",
       "\n",
       "$method\n",
       "[1] \"shell\"\n",
       "\n",
       "$app_name\n",
       "[1] \"sparklyr\"\n",
       "\n",
       "$config\n",
       "$config$spark.env.SPARK_LOCAL_IP.local\n",
       "[1] \"127.0.0.1\"\n",
       "\n",
       "$config$sparklyr.connect.csv.embedded\n",
       "[1] \"^1.*\"\n",
       "\n",
       "$config$spark.sql.legacy.utcTimestampFunc.enabled\n",
       "[1] TRUE\n",
       "\n",
       "$config$sparklyr.connect.cores.local\n",
       "[1] 16\n",
       "\n",
       "$config$spark.sql.shuffle.partitions.local\n",
       "[1] 16\n",
       "\n",
       "$config$sparklyr.shell.name\n",
       "[1] \"sparklyr\"\n",
       "\n",
       "$config$`sparklyr.shell.driver-memory`\n",
       "[1] \"2g\"\n",
       "\n",
       "attr(,\"config\")\n",
       "[1] \"default\"\n",
       "attr(,\"file\")\n",
       "[1] \"/usr/local/spark-3.0.1-bin-hadoop3.2/R/lib/sparklyr/conf/config-template.yml\"\n",
       "\n",
       "$state\n",
       "<environment: 0x55ed4bcb46d0>\n",
       "\n",
       "$extensions\n",
       "$extensions$jars\n",
       "character(0)\n",
       "\n",
       "$extensions$packages\n",
       "character(0)\n",
       "\n",
       "$extensions$initializers\n",
       "list()\n",
       "\n",
       "$extensions$catalog_jars\n",
       "character(0)\n",
       "\n",
       "$extensions$repositories\n",
       "character(0)\n",
       "\n",
       "\n",
       "$spark_home\n",
       "[1] \"/usr/local/spark-3.0.1-bin-hadoop3.2\"\n",
       "\n",
       "$backend\n",
       "A connection with                              \n",
       "description \"->localhost:8881\"\n",
       "class       \"sockconn\"        \n",
       "mode        \"wb\"              \n",
       "text        \"binary\"          \n",
       "opened      \"opened\"          \n",
       "can read    \"yes\"             \n",
       "can write   \"yes\"             \n",
       "\n",
       "$monitoring\n",
       "A connection with                              \n",
       "description \"->localhost:8881\"\n",
       "class       \"sockconn\"        \n",
       "mode        \"wb\"              \n",
       "text        \"binary\"          \n",
       "opened      \"opened\"          \n",
       "can read    \"yes\"             \n",
       "can write   \"yes\"             \n",
       "\n",
       "$gateway\n",
       "A connection with                              \n",
       "description \"->localhost:8880\"\n",
       "class       \"sockconn\"        \n",
       "mode        \"rb\"              \n",
       "text        \"binary\"          \n",
       "opened      \"opened\"          \n",
       "can read    \"yes\"             \n",
       "can write   \"yes\"             \n",
       "\n",
       "$output_file\n",
       "[1] \"/tmp/RtmpDi6DIp/file1d55ed70db_spark.log\"\n",
       "\n",
       "$sessionId\n",
       "[1] 9930\n",
       "\n",
       "$home_version\n",
       "[1] \"3.0.1\"\n",
       "\n",
       "attr(,\"class\")\n",
       "[1] \"spark_connection\"       \"spark_shell_connection\" \"DBIConnection\"         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt <- spark_read_parquet(sc,\n",
    "                        path = 'data/*')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
